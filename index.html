
<!DOCTYPE html>
<html>

<style>
body {
  margin-top: 30px;
  margin-bottom: 30px;
  margin-left: 50px;
  margin-right: 50px;
}
a {text-decoration : none; color : #003399;}
a:hover {text-decoration:underline; color: #CC0000; }
</style>


<script type='text/javascript'>

function getEmail() {
	var text = decode('216-166-201-44-224-220-225-232-212-203-44-220-201-223-203-213-214-206-44-44-203-222-224-224-166')

	var curText = document.getElementById('trick').innerHTML
	  if( text == curText ) {
		document.getElementById('trick').innerHTML = '';
	  }else{
		document.getElementById('trick').innerHTML = text
	  }
}

function random(s, N) {
    var x = Math.sin(s) * 10000;
    var r = x - Math.floor(x);
	return Math.floor(r * N);
}

function swap(arr, i, j) {
	if(i == j) return;
	arr[i] ^= arr[j];
	arr[j] ^= arr[i];
	arr[i] ^= arr[j];
}

function getRndArr(N, seed) {
	var arr = new Array(N);
	var i, s;
	for(i = 0; i < N; i ++)
		arr[i] = i;
	for(s = seed, i = 0; i < N-1; i ++, s += 3)
	{
		L = N - i;
		ind = random(s, L);
		swap(arr, ind, L - 1);
	}
	return arr;
}

function trans(text) {
	arr = getRndArr(text.length, 7);
	var outtxt = '';
	for(var i = 0; i < text.length; i ++)
		outtxt += text[arr[i]];
	return outtxt;
}

function transback(text) {
	arr = getRndArr(text.length, 7)
	var out = new Array(text.length);
	for(var i = 0; i < text.length; i ++)
		out[arr[i]] = text[i];
	return out.join('');
}

function num2strbase(num, base) {
	if(num == 0) return '0';
	var digits = new Array();
	for( ; num > 0; num = parseInt(num / base))
		digits.push(num % base);
	digits.reverse();
	return digits.join('');
}

function str2numbase(str, base) {
	var sum = 0;
	for(var i = 0; i < str.length; i ++)
		sum = sum * base + (str[i].charCodeAt() - '0'.charCodeAt());
	return sum;
}

function alpha2code(str, base) {
	var codes = new Array();
	for(var i = 0; i < str.length; i ++)
		codes.push(num2strbase(str[i].charCodeAt(), base));
	return codes.join('-');
}

function code2alpha(code, base) {
	var codes = code.split('-');
	str = '';
	for(var i = 0; i < codes.length; i ++)
		str += String.fromCharCode(str2numbase(codes[i], base));
	return str;
}

function decode(code) {
	alpha = code2alpha(code, 7);
	return transback(alpha);
}

function encode(str) {
	tmp = trans(str);
	return alpha2code(tmp, 7);
}

function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
  }
  dest.blur();
}


</script>

<head>
    <title>Xingxing Zhang's Homepage</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="keywords" content="Xingxing Zhang, Xingxing Zhang Edinburgh University">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Xingxing Zhang, Ph.D. student in Informatics at University of Edinburgh.">
</head>

<body>

<table cellpadding="20" style="font-size: 17px">
<tr>
<td><img src='images/me.png' alt="Xingxing Zhang"/ width = 200></td>
<td> 
<h1> Xingxing Zhang <span style="font-family:STFangsong; font-size:20pt"> (张星星)</span></h1>
<p>Pronounced as <I>Shingshing Zhang</I></p>
<!-- <font size="+1"><b>Ph.D. Candidate</b> in <b>School of Informatics</b>, <b>University of Edinburgh</b> --> <br> <br>

<table>
<tr>
	<!-- <td><b>E-mail</b>:</td> -->
	<td><button onclick="getEmail()"><b>E-mail:</b></button></td>
	<td><span id="trick" style="color: #000000; font-family: sans-serif, arial, helvetica;"></span></td>
<tr>

<tr>
	<td valign="top"><b>Address</b>: &nbsp;</td>
	<td>
		Building 2, No. 5 Dan Ling Street,  <br>
		Haidian District, <br>
		Beijing , 100080<br>
		China <br>
	</td>
</tr>
</table>
 
</td>
</tr>
</table>

<p>
Hi! I am a researcher at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> (MSRA). <br> 
Before joining MSRA, I obtained my Ph.D from <a href="http://www.ilcc.inf.ed.ac.uk/">ILCC</a>, <a href = "http://www.ed.ac.uk/schools-departments/informatics/"> School of Informatics</a>, <a href = "http://www.ed.ac.uk/home"> The University of Edinburgh</a> on Sep 2017, <br>
where I was working on Natural Language Generation and supervised by Prof. <a href="http://homepages.inf.ed.ac.uk/mlap/">Mirella Lapata</a> and Prof. <a href="http://alopez.github.io/">Adam Lopez</a>. <br>
</p>

<p><I>I am always looking for highly motivated interns to work with me on NLP tasks (e.g., text generation and pre-training). Feel free to drop me an email, if you are interested.</I></p>


<h2> Research Interests </h2>

<!-- Natural Language Generation, Language Modeling, Sequence to Sequence Learning, Attention Models, Structure Prediction with Neural Networks -->
<ul>
<li>Natural Language Generation</li>
	<ul>
		<li>Text Summarization</li>
		<li>Sequence-to-Sequence Pre-training</li>
		<li>Unsupervised Text Generation</li>
		<li>Sentence Simplification/Compression</li>
	</ul>
</ul>

<h2> Preprints </h2>
<ul>
    <li> <b><a href="https://arxiv.org/abs/2109.03481">Sequence Level Contrastive Learning for Text Summarization</a></b><br> Shusheng Xu, <b>Xingxing Zhang</b>, Yi Wu, Furu Wei. AriXv 2021</li>
    <br>
    <li> <b><a href="https://arxiv.org/abs/2106.03441">Attention Temperature Matters in Abstractive Summarization Distillation</a></b><br> Shengqiang Zhang, <b>Xingxing Zhang</b>, Hangbo Bao, Furu Wei. AriXv 2021</li>
</ul>

<h2> Publications </h2>
<a target="_blank" href="https://scholar.google.com/citations?user=5yX53usAAAAJ">Google Scholar</a>
<ul>
    <li> <b><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.161/">Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers</a></b><br> Shusheng Xu, <b>Xingxing Zhang</b>, Yi Wu, Furu Wei and Ming Zhou. In <I>Findings</I> of EMNLP 2020.</li>
    [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.161.bib">bib</a>] [<a href="https://arxiv.org/abs/2010.08242">arXiv</a>] [<a href="https://github.com/xssstory/STAS">code</a>]
  <br>
  <br>
    <li> <b><a href="https://www.aclweb.org/anthology/2020.emnlp-main.297/">Pre-training for Abstractive Document Summarization by Reinstating Source Text</a></b><br> Yanyan Zou, <b>Xingxing Zhang</b>, Wei Lu, Furu Wei and Ming Zhou. In EMNLP 2020.</li>
    [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.297.bib">bib</a>] [<a href="https://arxiv.org/abs/2004.01853">arXiv</a>] [<a href="https://github.com/zoezou2015/abs_pretraining">code</a>]
  <br>
  <br>
  <li> <b><a href="">Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction</a></b><br> Mengyun Chen, Tao Ge, <b>Xingxing Zhang</b>, Furu Wei and Ming Zhou. In EMNLP 2020.</li>
  <br>
  <br>
    <li> <b><a href="">DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</a></b><br> Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, <b>Xingxing Zhang</b>, Yue Hu, Qi Wu. In AAAI 2020.</li>
   [<a href="https://arxiv.org/abs/1911.07251">arXiv</a>]
  <br>
  <br>
    <li> <b><a href="https://www.aclweb.org/anthology/P19-1499">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</a></b><br> <b>Xingxing Zhang</b>, Furu Wei and Ming Zhou. In ACL 2019.</li>
    [<a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1499.bib">bib</a>] [<a href="https://arxiv.org/abs/1905.06566">arXiv</a>] [<a href="res/slides/hibert-acl2019.pdf">slides</a>] [<a href="hibert.html">project page</a>]
  <br>
  <br>
    <li> <b><a href="https://www.aclweb.org/anthology/P19-1609">Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study</a></b><br> Tao Ge, <b>Xingxing Zhang</b>, Furu Wei and Ming Zhou. In ACL 2019.</li>
  [<a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1609.bib">bib</a>]
  <br>
  <br>
    <li> <b><a href="http://aclweb.org/anthology/D18-1088">Neural Latent Extractive Document Summarization</a></b><br> <b>Xingxing Zhang</b>,  Mirella Lapata, Furu Wei and Ming Zhou. In EMNLP 2018 (<I>short paper</I>).</li>
	[<a href="https://aclanthology.info/papers/D18-1088/d18-1088.bib">bib</a>] [<a href="https://arxiv.org/abs/1808.07187">arXiv</a>] [<a href="latent.html">resource</a>]
  <br>
  <br>
  <li> <b><a href="https://www.era.lib.ed.ac.uk/handle/1842/28930">Natural Language Generation as Neural Sequence Learning and Beyond</a></b><br> <b>Xingxing Zhang</b>. Ph.D Thesis, 2017.</li> 
  [<a href="">bib</a>] [<a href="https://github.com/XingxingZhang">code & data</a>] [<a href="">slides</a>]
  <br>
  <br>
  <li> <b><a href="https://www.aclweb.org/anthology/K17-3010">UParse: the Edinburgh system for the CoNLL 2017 UD shared task</a></b><br> Clara Vania, <b>Xingxing Zhang</b>, Adam Lopez. In CoNLL 2017 Shared Task.</li> 
  [<a href="https://www.aclweb.org/anthology/K17-3010.bib">bib</a>][arXiv]
  <br>
  <br>
  <li> <b><a href="http://aclweb.org/anthology/D/D17/D17-1062.pdf">Sentence Simplification with Deep Reinforcement Learning</a></b><br> <b>Xingxing Zhang</b> and Mirella Lapata. In EMNLP 2017.</li> 
  [<a href="http://aclanthology.info/papers/D17-1063/d17-1062.bib">bib</a>] [<a href="https://arxiv.org/abs/1703.10931">arXiv</a>] [<a href="https://github.com/XingxingZhang/dress">code & data</a>] [<a href="res/slides/dress-emnlp2017.pdf">slides</a>]
  <br>
  <br>
  <li> <b><a href="http://aclweb.org/anthology/E17-1063">Dependency Parsing as Head Selection</a></b><br> <b>Xingxing Zhang</b>, Jianpeng Cheng and Mirella Lapata. In EACL 2017.</li> 
  [<a href="http://aclweb.org/anthology/E17-1063.bib">bib</a>] [<a href="http://arxiv.org/abs/1606.01280">arXiv</a>] [<a href="resources.html">resource</a>] [<a href="https://github.com/XingxingZhang/dense_parser">code</a>] [<a href="res/slides/dense.pdf">slides</a>]
  <br>
  <br>
  <li> <b><a href="http://aclweb.org/anthology/N/N16/N16-1035.pdf">Top-down Tree Long Short-Term Memory Networks</a></b><br> <b>Xingxing Zhang</b>, Liang Lu and Mirella Lapata. In NAACL 2016.</li> 
  [<a href="http://aclweb.org/anthology/N/N16/N16-1035.bib">bib</a>] [<a href="http://arxiv.org/abs/1511.00060">arXiv</a>] [<a href="https://github.com/XingxingZhang/td-treelstm">code</a>] [<a href="res/slides/treelstm.pdf">slides</a>]
  <br>
  <br>

  <li> <b><a href="http://homepages.inf.ed.ac.uk/llu/pdf/llu_icassp16.pdf">On Training the Recurrent Neural Network Encoder-Decoder for Large Vocabulary End-to-End Speech Recognition</a></b><br> Liang Lu, <b>Xingxing Zhang</b> and Steve Renals. In ICASSP 2016.</li> 
  <br>

  <li> <b><a href="http://homepages.inf.ed.ac.uk/llu/pdf/liang_is15a.pdf">A Study of the Recurrent Neural Network Encoder-Decoder for Large Vocabulary Speech Recognition</a></b><br> Liang Lu, <b>Xingxing Zhang</b>, Kyunghyun Cho and Steve Renals. In INTERSPEECH 2015.</li> 
  <br>

  <li> <b><a href="http://aclweb.org/anthology/D/D14/D14-1074.pdf">Chinese Poetry Generation with Recurrent Neural Networks</a></b><br> <b>Xingxing Zhang</b> and Mirella Lapata. In EMNLP 2014.</li> 
    [<a href="http://aclweb.org/anthology/D/D14/D14-1074.bib">bib</a>] [<a href="http://homepages.inf.ed.ac.uk/mlap/index.php?page=resources">data</a>] [<a href="https://github.com/XingxingZhang/rnnpg">code</a>]<br>

</ul>

<ul>
<li> 
<b><a href="http://www.aclweb.org/anthology/P13-2141">Towards Accurate Distant Supervision for Relational Facts Extraction</a></b><br>
<b>Xingxing Zhang</b>, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen and Zhifang Sui. In ACL 2013 (<I>short paper</I>). 
</li> 
<br>

<li> <b><a href="res/papers/CCL_2013.pdf">Learning to Extract Attribute Values from a Search Engine with Few Examples</a></b><br> <b>Xingxing Zhang</b>, Tao Ge and Zhifang Sui. In <I>Chinese Computational Linguistics</I> (CCL) 2013. 
</li>
<br>

<li>
	<b><a href="res/papers/semantic_class_learning.pdf">Semantic Class Learning with Deep Coordinate Structures in Web Pages</a></b><br>
	<b>Xingxing Zhang</b> and Zhifang Sui. <I>Journal of Computational Information Systems</I> (JCIS) 8:3 (2012) 1245-1254.
</li>

</ul>


<h2> Software </h2>
<ul>
  <li><a href="https://github.com/XingxingZhang/dress">Dress Simplification Model</a>: <b>DRESS</b>: <b>D</b>eep <b>RE</b>inforcement <b>S</b>entence <b>S</b>implification Model (in Lua with Torch)</li>
  <li><a href="https://github.com/XingxingZhang/dense_parser">DeNSe Parser</a>: A Neural Dependency Parser (in Lua with Torch)</li>
  <li><a href="https://github.com/XingxingZhang/td-treelstm">TD-TreeLSTM</a>: Top-down Tree Long Short-Term Memory Networks (in Lua with Torch)</li>
  <li><a href="https://github.com/XingxingZhang/rnnpg">RNNPG</a>: A (Hierarchical) Recurrent Neural Network based Chinese Poetry Generator (in C++)</li>
</ul>

<h2> Professional Activities </h2>
<ul>
    <li>Area Chair for ACL 2021</li>
</ul>

<hr>
Last updated: May 21, 2021

</body>
</html>



